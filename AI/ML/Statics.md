# 확률과 나이브베이즈(Naive Bayesian)

## 확률론
### 빈도주의자 vs 베이즈주의자
* 빈도주의자: 동전을 충분히 크게 던졌을 때 앞면이 50%, 뒷면이 50% 나온다. 
    - => 사건이 무한히 크게 발생할 때 사건의 확률을 정확히 정할 수 있다.
* 베이즈주의자: 동전 던지기의 결과가 앞면이 나올 것이라는 **확신/믿음**이 50%이다.
    - => 동전을 던지기 전에, 그 동전에 대한 정보를 알고있다면 사전 정보를 확률에 반영할 수 있다.
* 고등과정에서 배우는 확률은 빈도주의자적인 확률이다.

## 베이즈법칙
```
P(A|X) = P(X|A)P(A) / P(X)
```
* P(X|A): P(X) given A, A라는 조건이 있을 때 X일 확률.
* 예제: 암 검사 키트
* <img src=https://user-images.githubusercontent.com/53554014/88652777-38224380-d106-11ea-98d0-080164e361e0.jpg width=70% height=70% title="확률 계산"></src>

## 나이브 베이즈 분류기
* 분류(classification): 주어진 데이터가 어떤 클래스에 속하는지 알아내는 작업
* <img src=https://user-images.githubusercontent.com/53554014/88652598-04471e00-d106-11ea-8ef8-504848887b35.png width=70% height=70% title="기계학습"></src>
* 두 사건이 있을 때 P(A|X)와 P(B|X) 중 어느 것이 더 큰가?
```
P(A|X) : P(B|X) = P(X|A)P(A) / P(X) : P(X|B)P(B) / P(X)
```
* ![image](https://user-images.githubusercontent.com/53554014/88670176-b6d7aa80-d11f-11ea-87d4-acd61457112a.png)
* P(X|B): likelihood
* P(B): 사전(prior)확률
* P(B|X): 사후(posterior)확률

### 실습: 나이브 베이즈를 이용한 감정분류 모델 학습
* 긍정적인 문서 2000개와 부정적인 문서 2000개가 있다. *마음이 따뜻해지는 최고의 영화*와 같은 특정 단어가 입력되었을 때, 해당 문장이 긍정적인지 부정적인지 계산.
    1. Traning: 각각의 문서 셋들에서 나오는 단어의 빈도 수 측정
    2. 부정과 긍정 모델에서 '최고의'와 같이 특정 단어가 단어가 나올 확률 계산.
    3. 같은 방법으로 문장을 이루는 모든 단어의 확률을 계산
    4. 나이브 베이즈 계산
    5. 결론적으로 해당 문장이 긍정인지 부정인지 계산할 수 있다.
* 어떤 문장의 단어가 모델에 존재하지 않을 경우?
    - 해당 단어를 0은 아니지만 아주아주 작은 수의 상수로 둔다.(ex.0.000000000001)